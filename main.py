import os
import logging
from dotenv import load_dotenv

# Google GenAI SDK (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gemini 3 Thinking Mode)
from google import genai
from google.genai import types

# LlamaIndex Components (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Memory Layer)
from llama_index.core import VectorStoreIndex, KnowledgeGraphIndex, StorageContext, Settings, Document
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.graph_stores.neo4j import Neo4jGraphStore
from llama_index.vector_stores.qdrant import QdrantVectorStore
import qdrant_client

# --- Configuration ---
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASS = os.getenv("NEO4J_PASSWORD", "password123") 
NEO4J_URL = os.getenv("NEO4J_URL", "bolt://localhost:7687")
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")

if not GEMINI_API_KEY:
    raise ValueError("‚ùå Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env")

print(f"‚úÖ Config Loaded: Neo4j user={NEO4J_USER} at {NEO4J_URL}")

# --- 1. Setup Local Brain (Ollama for Embedding) ---
# ‡πÉ‡∏ä‡πâ Ollama ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤ Embedding ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö Data ‡πÑ‡∏ß‡πâ‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß
Settings.embed_model = OllamaEmbedding(model_name="nomic-embed-text")
Settings.llm = None # ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ LlamaIndex ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏ï‡∏£‡∏á‡πÜ ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ú‡πà‡∏≤‡∏ô SDK Gemini 3 ‡πÅ‡∏ó‡∏ô

# --- 2. Connect to Agentic Memory (Graph + Vector) ---
# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Graph DB (Neo4j)
graph_store = Neo4jGraphStore(
    username=NEO4J_USER,
    password=NEO4J_PASS,
    url=NEO4J_URL,
)

# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Vector DB (Qdrant)
client = qdrant_client.QdrantClient(url=QDRANT_URL)
vector_store = QdrantVectorStore(client=client, collection_name="research_memory")

storage_context = StorageContext.from_defaults(
    graph_store=graph_store,
    vector_store=vector_store
)

# --- Function: ‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Ingestion) ---
def ingest_data(text_content, doc_id):
    print(f"üß† Generating Memory for: {doc_id}...")
    documents = [Document(text=text_content, id_=doc_id)]
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Graph Index (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå) ‡πÅ‡∏•‡∏∞ Vector Index (‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢)
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Graph ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ LLM ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏Å‡∏±‡∏î Entity ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 
    # ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏™‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏£‡∏≤‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ Ollama (llama3) ‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏Å‡∏±‡∏î Entity ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ
    index = KnowledgeGraphIndex.from_documents(
        documents,
        storage_context=storage_context,
        max_triplets_per_chunk=2,
        include_embeddings=True # Hybrid Search
    )
    print("‚úÖ Memory Stored locally!")
    return index

# --- Function: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Gemini 3 ‡∏°‡∏≤‡∏Ñ‡∏¥‡∏î (The Thinking Process) ---
def ask_gemini_thinking(query):
    # 1. Recall: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Local Memory ‡∏Å‡πà‡∏≠‡∏ô
    # (‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Retrieve ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏ä‡∏ß‡πå‡∏™‡πà‡∏ß‡∏ô Thinking)
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏£‡∏≤‡∏î‡∏∂‡∏á Context ‡∏à‡∏≤‡∏Å Graph/Vector ‡∏°‡∏≤‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß:
    retrieved_context = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Neo4j/Qdrant..." 

    print("ü§î Gemini 3 is thinking...")
    client = genai.Client(api_key=GEMINI_API_KEY)
    
    # Config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thinking Mode
    config = types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(include_thoughts=True),
        thinking_level="HIGH", # Maximum reasoning
        temperature=1.0 
    )

    prompt = f"""
    Context from my local database:
    {retrieved_context}

    User Question: {query}
    
    Please analyze the context deeply using your thinking process. 
    Identify connections, contradictions, or hidden patterns.
    """

    response = client.models.generate_content(
        model="gemini-2.0-flash-thinking-exp", # ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠ model ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö (‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏ô Docs ‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ)
        contents=prompt,
        config=config
    )

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î (Thoughts)
    for part in response.candidates[0].content.parts:
        if part.thought:
            print(f"\n--- üí≠ Thoughts Process ---\n{part.text}\n")
        else:
            print(f"\n--- üìù Final Answer ---\n{part.text}")

# --- Main Execution ---
if __name__ == "__main__":
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ (‡∏ó‡∏≥‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà)
    sample_text = "LightRAG ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Graph Database ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö Vector. Gemini 3 ‡∏°‡∏µ Thinking Mode ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏°‡∏≤‡∏Å."
    ingest_data(sample_text, "doc_001")

    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    ask_gemini_thinking("LightRAG ‡∏Å‡∏±‡∏ö Gemini 3 ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?")