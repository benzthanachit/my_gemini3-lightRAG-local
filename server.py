import os
import sys
import time
import uvicorn
from typing import List, Optional
from pydantic import BaseModel
from dotenv import load_dotenv
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks
from google import genai
from google.genai import types

from llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex, StorageContext, Settings, Document, load_index_from_storage
# üëá [‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ 1] Import Gemini Embedding
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.graph_stores.neo4j import Neo4jGraphStore
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.llms.gemini import Gemini
import qdrant_client

# --- 1. Configuration & Setup ---
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASS = os.getenv("NEO4J_PASSWORD")
NEO4J_URL = os.getenv("NEO4J_URL", "bolt://localhost:7687")
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
PERSIST_DIR = "/app/storage_metadata"

GLOBAL_INDEX = None
STORAGE_CONTEXT = None

# --- 2. Lifespan Logic ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global GLOBAL_INDEX, STORAGE_CONTEXT
    print("üöÄ Server Starting... Connecting to Databases...")
    try:
        # üëá [‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ 2] ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ Gemini Embedding (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Qdrant ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤/‡∏Ñ‡πâ‡∏≤‡∏á)
        print("üîå Using Gemini Embedding (models/text-embedding-004)...")
        Settings.embed_model = GeminiEmbedding(
            model_name="models/text-embedding-004",
            api_key=GEMINI_API_KEY
        )

        # üëá [‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ 3] ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ 2.5 Flash
        Settings.llm = Gemini(model="models/gemini-2.5-flash", api_key=GEMINI_API_KEY)

        graph_store = Neo4jGraphStore(username=NEO4J_USER, password=NEO4J_PASS, url=NEO4J_URL)
        client = qdrant_client.QdrantClient(url=QDRANT_URL)
        
        # üëá [‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ 4] ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ Collection ‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå
        vector_store = QdrantVectorStore(client=client, collection_name="research_memory_gemini")
        
        STORAGE_CONTEXT = StorageContext.from_defaults(graph_store=graph_store, vector_store=vector_store)
        print("‚úÖ Database Connected!")

        if os.path.exists(PERSIST_DIR) and os.listdir(PERSIST_DIR):
            try:
                GLOBAL_INDEX = load_index_from_storage(STORAGE_CONTEXT, persist_dir=PERSIST_DIR)
                print("üéâ Success! Memory Loaded from Disk.")
            except Exception:
                GLOBAL_INDEX = None
        else:
            print("‚ÑπÔ∏è No existing memory found (General Chat Mode enabled).")
            GLOBAL_INDEX = None
            
    except Exception as e:
        print(f"‚ùå Startup Error: {e}")

    yield
    print("üõë Server Shutting down...")

app = FastAPI(title="Gemini 3 Research Agent API", openapi_url="/v1/openapi.json", lifespan=lifespan)

# --- 3. Helper Functions ---
def retrieve_context(query_text: str):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Index ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ None"""
    if GLOBAL_INDEX is None:
        return None
    try:
        retriever = GLOBAL_INDEX.as_retriever(similarity_top_k=3, vector_store_query_mode="default")
        nodes = retriever.retrieve(query_text)
        if not nodes:
            return None
        return "\n\n".join([n.get_content() for n in nodes])
    except Exception:
        return None

def ask_gemini_thinking(query: str, context: Optional[str] = None):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Context ‡∏Å‡πá‡πÉ‡∏ä‡πâ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡∏Ñ‡∏∏‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥"""
    client = genai.Client(api_key=GEMINI_API_KEY)
    
    config = types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(include_thoughts=True),
        temperature=1.0 
    )

    # Logic ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Prompt ‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå
    if context:
        print("üí° Using RAG Mode (Memory Found)")
        prompt = f"""
        You are an advanced AI Researcher Assistant.
        Use the following Context from my local database to answer the user's question.
        
        --- Context ---
        {context}
        ---------------
        
        User Question: {query}
        
        Analyze the context deeply using your thinking process.
        If the context is relevant, use it.
        """
    else:
        print("üó£Ô∏è Using General Chat Mode (No Memory)")
        prompt = f"""
        You are a helpful and intelligent AI Assistant (Gemini 3).
        User Question: {query}
        
        Answer the user's question using your general knowledge and thinking process.
        """

    try:
        # ‡πÉ‡∏ä‡πâ Gemini 3 Pro ‡∏Ñ‡∏¥‡∏î (‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Ç‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏≠‡∏á)
        response = client.models.generate_content(
            model="gemini-3-pro-preview",
            contents=prompt,
            config=config
        )
        
        thought_text = ""
        final_answer = ""
        for part in response.candidates[0].content.parts:
            if hasattr(part, 'thought') and part.thought:
                thought_text += part.text
            else:
                final_answer += part.text
        
        full_response = ""
        if thought_text:
            full_response += f"> **üß† Thinking Process:**\n> {thought_text.replace(chr(10), chr(10)+'> ')}\n\n---\n\n"
        full_response += final_answer
        return full_response
    except Exception as e:
        return f"Error from Gemini: {str(e)}"

# --- 4. API Endpoints ---
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    model: Optional[str] = "gemini-3-researcher"

class IngestRequest(BaseModel):
    folder_path: str = "./data"

@app.get("/v1/models")
async def list_models():
    return {"object": "list", "data": [{"id": "gemini-3-researcher", "object": "model", "created": int(time.time()), "owned_by": "benzondata"}]}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    user_query = request.messages[-1].content
    print(f"üì© Received: {user_query}")

    # 1. ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏π‡∏Å‡πà‡∏≠‡∏ô
    context_text = retrieve_context(user_query)
    
    # 2. ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini ‡∏Ñ‡∏¥‡∏î (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≠ context ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏Å‡πá‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏´‡∏°‡∏î)
    reply = ask_gemini_thinking(user_query, context_text)

    return {
        "id": "chatcmpl-" + str(int(time.time())),
        "object": "chat.completion",
        "created": int(time.time()),
        "model": request.model,
        "choices": [{"index": 0, "message": {"role": "assistant", "content": reply}, "finish_reason": "stop"}],
        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    }

@app.post("/ingest")
async def trigger_ingest(request: IngestRequest, background_tasks: BackgroundTasks):
    def process_ingestion(path):
        global GLOBAL_INDEX
        print(f"üìÇ Starting Ingestion from: {path}...")
        try:
            if not os.path.exists(path): return
            documents = SimpleDirectoryReader(path).load_data()
            if not documents: return
            
            # ‡πÉ‡∏ä‡πâ Gemini Embedding ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á Graph ‡πÅ‡∏•‡∏∞ Vector
            print(f"üìÑ Building Graph & Vector for {len(documents)} docs (Gemini)...")
            GLOBAL_INDEX = KnowledgeGraphIndex.from_documents(
                documents, storage_context=STORAGE_CONTEXT, max_triplets_per_chunk=2, include_embeddings=True
            )
            if not os.path.exists(PERSIST_DIR): os.makedirs(PERSIST_DIR)
            GLOBAL_INDEX.storage_context.persist(persist_dir=PERSIST_DIR)
            print("‚úÖ Ingestion Complete!")
        except Exception as e:
            print(f"‚ùå Ingestion Failed: {e}")

    background_tasks.add_task(process_ingestion, request.folder_path)
    return {"status": "Ingestion started", "folder": request.folder_path}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)